\documentclass{acm_proc_article-sp-sigmod07}


\begin{document}

\title{Protein subcelluar localization}
\numberofauthors{2}
\author{Luca Gasparella, Enrico Sartori}

\maketitle

\begin{abstract}
This paper present a classifier, based on several SVMs, that is able to 
recognize the location of a protein in the cell given its amino-acids sequence.
The main objective of the paper is to illustrate the procedure adopted to 
retrieve the best parameter settings, for each SVM, in order to maximize
the overall reliability.
\end{abstract}

\section{INTRODUCTION}
The main objective of this work is to predict the position of a protein with 
respect to the cell and its structure. The target is to localize the protein 
considering the cell environment, in particular, define if it belongs into 
one of the following possibilities: the Cytoplasm, the Nucleus, 
the Mitochondria or if it's Secreted by the cell.

To perform the classification, a one-vs-all approach was chosen building a
classifier composed by four classifiers, based on Support Vector Machines (SVM),
each of them specialized one of the possible localization where the protein can
appears. The results of the protein's classification is the highest probability 
of membership to a classifier.

In order to achieve best performance, all the SVMs have been tuned through 
cross validation operating on two major parameters. The best set of parameter
which maximize the F-measure is considered the final choice to train a
SVM.

Several experiments have been performed in order to evaluate the range of 
possible training parameters which affects the single SVM, a survey about
all the computation has been conducted to discuss the obtained results.


\section{PROBLEM}
The problem of Protein subcellar localization aims to guess the position of a 
given protein in the cellular context knowing its structure. There are four 
possible place where a protein may lies once synthesized: Cytoplasm, Nucleus, 
Mitochondrion and Secretory.  
Each protein is composed by a sequence of amino-acids, that technically is a 
string of letters which represent its one-dimensional structure. 

The provided  dataset is formed by animal proteins which belong to 
different Natural Kingdoms. Changing the Kingdom changes also the position
of the protein in the cell environment. However, by the scope of the current
classification, it is no important knowing what Kingdom the cell belongs to.
The dataset is composed by four dataset file, related t  the sequences of
amino-acids coming from a specific location.

To build the classifier it has been chosen the Support Vector approach, defining
a trained SVM for each localization. The classifier will query the four SVM and
it take the prediction which has the major probability to belong into the class.


\section{PREPROCESSING DATA}
%Description of phases conducted to prepare data for learning algorithms
Data are given in a plain text file, where an entry is composed by two rows.
The first row is the name of the animal studied, while the second is the
one-dimensional protein structure as following:

\begin{verbatim}
>ADA2_HUMAN
MDRLGSFSNDPSDKPPCRGCSSYLMEPYIKCAECGPPPFFLCLQCFTRGFE
YKKHQSDHTYEIMTSDFPVLDPSWTAQEEMALLEAVMDCGFGNWQDVANQM
CTKTKEECEKHYMKHFINNPLFASTLLNLKQAEEAKTADTAIPFHSTDDPP
RPTFDSLLSRDMAGYMPARADFIEEFDNYAEWDLRDIDFVEDDSDILHALK
MAVVDIYHSRLKERQRRKKIIRDHGLINLRKFQLMERRYPKEVQDLYETMR
RFARIVGPVEHDKFIESHALEFELRREIKRLQEYRTAGITNFCSARTYDHL
KKTREEERLKRTMLSEVLQYIQDSSACQQWLRRQADIDSGLSPSIPMASNS
GRRSAPPLNLTGLPGTEKLNEKEKELCQMVRLVPGAYLEYKSALLNECNKQ
GGLRLAQARALIKIDVNKTRKIYDFLIREGYITKG
\end{verbatim}

Since the main objective is to maximize the result, it has been applied the 
cross-validation methodology, splitting each dataset in three compounds: the
training, the validation and the testing. The procedure effectively reads each 
line of the dataset and in the training phase choose randomly in which fold it 
will belong.

In order to elaborate data, the protein samples need to be acquired in a easy 
computable way, that is splitting each one-dimensional sequence into substrings
of different dimension. The data structure obtained is effectively a 
string-kernel. Thus the substrings are sorted and are univocally 
identified through a numerical id which has associated the number of its 
occurrences into the protein.

\subsection{String Kernel}
The preprocessing approach implemented in the project is the string
kernel. In fact the algorithm retrieves all the substrings of a given
dimension, selected by the user, present in the dataset.
Each substring is a feature of the examples, and its value is the number
of occurrences found in the protein. 

The entire set of substring is sorted alphabetically and an unique id is
assigned to each of them. 
The representation of a protein is a vector containing the number of
occurrences of the different substrings.

In order to optimize the time needed for converting a string describing a
protein into its corresponding vector the actual implementation relies on
hashsets (Python dictionaries) for storing the list of substrings and
retrieve their ids easily.


\section{ALGORITHMS}
%Description of algorithms adopted
The global view is of a classifier that is actually composed by four SVM tuned to
achieve the best results in each of the possible prediction area. The final output
of the program is the output prediction of the SVM which achieves the greatest 
probability.

For the SVM the chosen is the implementation of the algorithm given by 
Chih-Chung Chang and Chih-Jen Lin \cite{libSVM} to take advantage by a well 
known working core. 
In particular, it has been chosen the C-SVM implementation with a Radial Basis 
Function kernel, in \ref{RBF}, since during the preliminary tests they give the best 
performances. 

\begin{equation}
 K(\bold{x}_i , \bold{x}_j ) = \exp{(-\gamma\|\bold{x}_i - \bold{x}j\|^2 )}, \gamma > 0
\label{RBF}
\end{equation}

The evaluation of the model obtained setting the \emph{C} parameter, for
the SVM, and the $ \gamma $ parameter, for the kernel, is performed maximizing 
the F-measure as in \ref{f_meas} obtained using the training set for learning and
the validation set to perform the measurements. The F-measure is a general index
that allows to maximize both Precision and Recall measurements, so through that is
assured the program will reach a good performance about the quality of the 
classification.

\begin{equation}
F-measure = 2 * \frac{Precision * Recall}{Precision + Recall}
\label{fmeas}
\end{equation}

The two parameters the algorithm must found are concerning the way the classifier
realize the hyperplane over a vector space of different thousands of dimensions.
Since there isn't a precise way to retrieve them, the way chosen perform a 
linear scanning over all the possible values in a good range; that is an interval
which is known to have good results. By theory, a great value of \emph{C} suggest
to have a harder margin since the cost of the errors is high; a theoretical good
value for $ \gamma $ is given by the formula \ref{gamma}

\begin{equation}
\frac{1}{DSdimension}
\label{gamma}
\end{equation}

where the \emph{DSdimension} is the number of samples of the dataset of the
problems. 


\subsection{One-vs-All approach}
The one-vs-all approach method consists to build one tuned SVM for each possible 
location in order maximize the probability of the prediction for given a sample
in that class.

During the validation phase, each single SVM iterate on a defined range of \emph{C} and
$ \gamma $ looking for that couple that obtain the best performances. Since each
trainig phase is time expensive, it has been decided to perform a single skim
of both ranges to find one possible couple and later take a closest range centered
on that result to finer the research.

Once the 

\section{SOFTWARE STRUCTURE}
The language chosen to implement the algorithm is python for its attitude to allow 
good interface to the oriented object programming and data structures.
Its major task based the work on the libSVM\cite[libSVM] that is written in C and 
wrapped into python through a support library.

The software is composed by two main executable programs and some modules.
The first executable\footnote{subcell-train.py}
works take as input the dataset directory and some parameters 
to conduct the experiment and so build the model, that is saved into the working 
directory once it has been tested. The second executable\footnote{subcell.py}
permit to load into memory the model and use it for a new testing session.

An outline of the possible options, for the ''subcell-train.py`` 
is provided by the following list:
\begin{description}
\item{-t} percentage of how many samples to reserve to build the training dataset, 
optional 
\item{-v} percentage of how many samples to reserve to build the validation dataset, 
mandatory
\item{-k} dimension of the k-grams, mandatory
\item{-C} fix a value for C parameter, optional
\item{-g} fix a value for gamma parameter, optional
\item{-n} number of interaction, optional
\item{-w} penalty weight for the negative samples, optional
\item{-m} output model name, mandatory
\item{-s} single run mode to test a copule of parameters
\end{description}


\section{WORKING METHOD}
%Description of experimental settings
In the beginning some general experiments have been run in order to test the
available kernels, but by the results obtained was clear that the RBF kernel
collected the main results. Hence it was chosen as fixed assumption to avoid
expensive useless iterations.

Several test was conducted to find the best range of iteration for the two
parameters, finding that is the following:

$$
	\bold{C} = [0.03125, 131072]
$$
$$
 	\bold{\gamma} = [0.00006, 0.000001]
$$

While for the \emph{C} parameter there wasn't a real deal to afford, the 
$ \gamma $ parameter is found to be around the theoretical value of \ref{gamma}

In the run experiments was considered a set build up of two, three and
five k-grams and the overall number of features was around six houndred and 
thirty thousand, for which the value expressed by \ref{gamma} is 0.0005.

In order to face the difficulty of prediction on two problematic locations,
which are Cytoplasm and Mitochondrion, the parameter \emph{weight} for the
negative examples was setted to different values, but the best result was 
obtained with a value equal ten.

\section{EXPERIMENTS}
%Results
The experiments was performed mainly considering the k-grams with two, three and
five letters together, five iterations and a penalty weight about ten.
The target was the trend of the F-measure in \ref{f_meas} depending the \emph{C}
parameter, so the plots are about it.

An observation is concerning the number of iterations, as well, the greater is 
the better, but due to time consuming issue they were reduced to five since it
is a good compromise by the heuristic. 

Another relevant consideration is about the penalty weight. The best performance
obtained have a weight of ten and was decided to keep the value to have a standard
comparison.


\begin{figure*}
\label{fig:K235W2N5}
\centering
\epsfig{file=plot/K235W2N5/K235W2N5-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/K235W2N5/K235W2N5-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/K235W2N5/K235W2N5-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/K235W2N5/K235W2N5-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 2; N = 5;}
\end{figure*}

\begin{figure*}
\label{fig:K235W5N5}
\centering
\epsfig{file=plot/K235W5N5/K235W5N5-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/K235W5N5/K235W5N5-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/K235W5N5/K235W5N5-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/K235W5N5/K235W5N5-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 5; N = 5;}
\end{figure*}

\begin{figure*}
\label{fig:K235W5N5-II}
\centering
\epsfig{file=plot/K235W5N5-II/K235W5N5-II-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/K235W5N5-II/K235W5N5-II-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/K235W5N5-II/K235W5N5-II-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/K235W5N5-II/K235W5N5-II-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 5; N = 5;}
\end{figure*}


\begin{figure*}
\label{fig:K235W5N5603010}
\centering
\epsfig{file=plot/K235W5N5603010/K235W5N5603010-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/K235W5N5603010/K235W5N5603010-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/K235W5N5603010/K235W5N5603010-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/K235W5N5603010/K235W5N5603010-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 5; N = 5; the training set is 60\%, the
valiation dataset is 30\% and the test dataset is 10\%;}
\end{figure*}

\begin{figure*}
\label{fig:fast_K235W10N7}
\centering
\epsfig{file=plot/fast_K235W10N7/fast_K235W10N7-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/fast_K235W10N7/fast_K235W10N7-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/fast_K235W10N7/fast_K235W10N7-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/fast_K235W10N7/fast_K235W10N7-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 10; N = 7; the training set is 20\%, the
validation set is 20\% and the test set is 60\%;}
\end{figure*}

\begin{figure*}
\label{fig:fast_K235W10N5ng}
\centering
\epsfig{file=plot/fast_K235W10N5ng/fast_K235W10N5ng-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/fast_K235W10N5ng/fast_K235W10N5ng-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/fast_K235W10N5ng/fast_K235W10N5ng-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/fast_K235W10N5ng/fast_K235W10N5ng-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 10; N = 5; gamma range of [0.000050,  0.001000];
the training set is 20\%, the validation set is 20\% and the testing set
is 60\%;}
\end{figure*}

\begin{figure*}
\label{fig:K235W10N5}
\centering
\epsfig{file=plot/K235W10N5/K235W10N5-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/K235W10N5/K235W10N5-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/K235W10N5/K235W10N5-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/K235W10N5/K235W10N5-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 10; N = 5; gamma range is
[0.000031,  0.500000]}
\end{figure*}

\begin{figure*}
\label{fig:K235W10N5G2}
\centering
\epsfig{file=plot/K235W10N5G2/K235W10N5G2-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/K235W10N5G2/K235W10N5G2-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/K235W10N5G2/K235W10N5G2-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/K235W10N5G2/K235W10N5G2-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 10; N = 5; gamma range is 
[0.000001,  0.000060]}
\end{figure*}

\begin{figure*}
\label{fig:K235W10N5G001-9}
\centering
\epsfig{file=plot/K235W10N5G001-9/K235W10N5G001-9-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/K235W10N5G001-9/K235W10N5G001-9-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/K235W10N5G001-9/K235W10N5G001-9-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/K235W10N5G001-9/K235W10N5G001-9-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 10; N = 5; gamma range is
[0.0000001, 0.00009]}
\end{figure*}

\begin{figure*}
\label{fig:K235W10N5G50-300}
\centering
\epsfig{file=plot/K235W10N5G50-300/K235W10N5G50-300-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/K235W10N5G50-300/K235W10N5G50-300-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/K235W10N5G50-300/K235W10N5G50-300-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/K235W10N5G50-300/K235W10N5G50-300-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 10; N = 5; gamma range is
[0.00005, 0.0003]}
\end{figure*}

\begin{figure*}
\label{fig:K235W10N5ng}
\centering
\epsfig{file=plot/K235W10N5ng/K235W10N5ng-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/K235W10N5ng/K235W10N5ng-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/K235W10N5ng/K235W10N5ng-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/K235W10N5ng/K235W10N5ng-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 10; N = 5; gamma range is
 [0.000001,  0.000060]}
\end{figure*}

\begin{figure*}
\label{fig:fast_K235W10N7bigC}
\centering
\epsfig{file=plot/fast_K235W10N7bigC/fast_K235W10N7bigC-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/fast_K235W10N7bigC/fast_K235W10N7bigC-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/fast_K235W10N7bigC/fast_K235W10N7bigC-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/fast_K235W10N7bigC/fast_K235W10N7bigC-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 10; N = 7; [0.031250, 262144]; training set is 
20\%, the validation set is 20\% and the test set is 60\%;}
\end{figure*}

\begin{figure*}
\label{fig:K235W15N5}
\centering
\epsfig{file=plot/K235W15N5/K235W15N5-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/K235W15N5/K235W15N5-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/K235W15N5/K235W15N5-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/K235W15N5/K235W15N5-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 15; N = 5;}
\end{figure*}

\begin{figure*}
\label{fig:K235W100N5}
\centering
\epsfig{file=plot/K235W100N5/K235W100N5-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/K235W100N5/K235W100N5-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/K235W100N5/K235W100N5-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/K235W100N5/K235W100N5-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 100; N = 5;}
\end{figure*}

\begin{figure*}
\label{fig:K235W1000N7}
\centering
\epsfig{file=plot/K235W1000N7/K235W1000N7-tuning-Cytoplasm.ps,scale=0.35}
\epsfig{file=plot/K235W1000N7/K235W1000N7-tuning-Nucleus.ps,scale=0.35}
\epsfig{file=plot/K235W1000N7/K235W1000N7-tuning-Mitochondrion.ps,scale=0.35}
\epsfig{file=plot/K235W1000N7/K235W1000N7-tuning-Secretory.ps,scale=0.35}
\caption{Experiment with K = 2,3,5; W = 1000; N = 7;}
\end{figure*}


\section{CONCLUSIONS}
* Differences by Nucleus e Secretory good results, altri due no; in Cytoplasm peggiore
*  

\bibliographystyle{abbrv}
\bibliography{report}

\newpage
\null
\newpage

\end{document}
